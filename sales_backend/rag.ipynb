{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6417d987",
      "metadata": {
        "id": "6417d987"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google import genai\n",
        "import vertexai\n",
        "from google.colab import auth\n",
        "\n",
        "PROJECT_ID = \"customer-team-hackathon-2025\"\n",
        "LOCATION = \"us-east4\"\n",
        "\n",
        "# Authenticate to Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7322dc52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7322dc52",
        "outputId": "c1c03c0b-94e6-4863-9f7d-074ff826dc38"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore\n",
        "from vertexai import rag\n",
        "\n",
        "\n",
        "\n",
        "EMBEDDING_MODEL = \"publishers/google/models/text-multilingual-embedding-002\"\n",
        "\n",
        "rag_corpus = rag.create_corpus(\n",
        "    display_name=\"sdr-chatbot-corpus\",\n",
        "    backend_config=rag.RagVectorDbConfig(\n",
        "        rag_embedding_model_config=rag.RagEmbeddingModelConfig(\n",
        "            vertex_prediction_endpoint=rag.VertexPredictionEndpoint(\n",
        "                publisher_model=EMBEDDING_MODEL\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "rag.list_corpora()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d97ba4",
      "metadata": {
        "id": "e7d97ba4"
      },
      "outputs": [],
      "source": [
        "# 1) (Colab) Authenticate\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# 2) Obtain ADC\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "creds, _ = google.auth.default(\n",
        "    scopes=['https://www.googleapis.com/auth/drive.readonly']\n",
        ")\n",
        "drive_svc = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "# 3) Correct folder ID\n",
        "FOLDER_ID = \"1qh3Gjo6UNrfZ6f6fILQqwj6wZgSRPxH1\"\n",
        "\n",
        "# 4) List files\n",
        "all_files, page_token = [], None\n",
        "while True:\n",
        "    resp = drive_svc.files().list(\n",
        "        q=f\"'{FOLDER_ID}' in parents and trashed = false\",\n",
        "        fields=\"nextPageToken, files(id, name, createdTime)\",\n",
        "        pageSize=1000,\n",
        "        pageToken=page_token\n",
        "    ).execute()\n",
        "    all_files.extend(resp.get('files', []))\n",
        "    page_token = resp.get('nextPageToken')\n",
        "    if not page_token:\n",
        "        break\n",
        "\n",
        "# 3. Sort & slice\n",
        "#    e.g. by creation date descending (newest first)\n",
        "all_files.sort(key=lambda f: f['createdTime'], reverse=True)\n",
        "top_files = all_files[:8000]\n",
        "\n",
        "# 4. Build Drive‐file URLs\n",
        "paths = [\n",
        "    f\"https://drive.google.com/file/d/{f['id']}\"\n",
        "    for f in top_files\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WlAeVExyH27f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlAeVExyH27f",
        "outputId": "80c935cf-3879-4eea-b3eb-54e1cb9feea3"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "\n",
        "# Utility to split a list into chunks of size n\n",
        "def chunked(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "# Suppose `paths` is your list of Drive URLs\n",
        "batch_size = 25\n",
        "num_batches = ceil(len(paths) / batch_size)\n",
        "\n",
        "for idx, batch in enumerate(chunked(paths, batch_size), start=1):\n",
        "    print(f\"Importing batch {idx}/{num_batches} ({len(batch)} files)…\")\n",
        "    result = rag.import_files(\n",
        "            rag_corpus.name,\n",
        "            paths=batch,\n",
        "            transformation_config=rag.TransformationConfig(\n",
        "                rag.ChunkingConfig(chunk_size=512, chunk_overlap=100),\n",
        "            ),\n",
        "            max_embedding_requests_per_min=1000,\n",
        "        )\n",
        "    print(f\"  → Imported {result.imported_rag_files_count} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-NQwO-G9Aa1s",
      "metadata": {
        "id": "-NQwO-G9Aa1s"
      },
      "source": [
        "# TRANSCRIPT UPLOAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22Xi5kzctGjk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Xi5kzctGjk",
        "outputId": "3fa86460-8fe6-4831-bc5e-a6274c4ae46e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "# It's highly recommended to use environment variables for your keys.\n",
        "ACCESS_KEY = \"\"\n",
        "SECRET_KEY = \"\"\n",
        "BASE_URL = \"\"\n",
        "OUTPUT_DIR = \"gong_transcripts\"\n",
        "RATE_LIMIT_DELAY = 0.2  # Seconds to wait between transcript requests\n",
        "FROM_DATETIME = \"2025-01-01T00:00:00-04:00\"\n",
        "TO_DATETIME = \"2025-08-01T23:59:59-04:00\"\n",
        "\n",
        "def fetch_transcripts_batch(cursor=None):\n",
        "    \"\"\"\n",
        "    POST to /v2/calls/transcript with optional cursor,\n",
        "    returns (records, callTranscripts).\n",
        "    \"\"\"\n",
        "    url = f\"{BASE_URL}/v2/calls/transcript\"\n",
        "    payload = {\n",
        "        \"filter\": {\n",
        "            \"fromDateTime\": FROM_DATETIME,\n",
        "            \"toDateTime\": TO_DATETIME,\n",
        "        }\n",
        "    }\n",
        "    if cursor:\n",
        "        payload[\"cursor\"] = cursor\n",
        "\n",
        "    resp = requests.post(\n",
        "        url,\n",
        "        auth=(ACCESS_KEY, SECRET_KEY),\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "        json=payload\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    return data[\"records\"], data.get(\"callTranscripts\", [])\n",
        "\n",
        "def format_and_write(transcript_obj):\n",
        "    \"\"\"\n",
        "    Given a single callTranscripts entry, format out its transcript\n",
        "    and write it to a text file.\n",
        "    \"\"\"\n",
        "    call_id = transcript_obj[\"callId\"]\n",
        "    lines = []\n",
        "    for utterance in transcript_obj.get(\"transcript\", []):\n",
        "        spk = utterance.get(\"speakerId\", \"Unknown\")\n",
        "        # each utterance may contain one or more sentences\n",
        "        text = utterance.get(\"text\") or \"\"\n",
        "        lines.append(f\"{spk}: {text}\")\n",
        "    body = \"\\n\".join(lines) or \"<no transcript text>\"\n",
        "\n",
        "    fn = os.path.join(OUTPUT_DIR, f\"gong_transcript_{call_id}.txt\")\n",
        "    with open(fn, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(body)\n",
        "    print(f\"  → Saved transcript for call {call_id} → {fn}\")\n",
        "\n",
        "def main():\n",
        "    if not all([ACCESS_KEY, SECRET_KEY, FROM_DATETIME, TO_DATETIME]):\n",
        "        raise RuntimeError(\"Make sure GONG_ACCESS_KEY, GONG_SECRET_KEY, GONG_WORKSPACE_ID, GONG_FROM and GONG_TO are set.\")\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"Fetching transcripts from {FROM_DATETIME} to {TO_DATETIME}...\")\n",
        "\n",
        "    cursor = None\n",
        "    batch_num = 1\n",
        "    total_fetched = 0\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\nBatch #{batch_num} (cursor={cursor})...\")\n",
        "        records, transcripts = fetch_transcripts_batch(cursor)\n",
        "        print(f\"  → {len(transcripts)} transcripts returned (totalRecords={records['totalRecords']})\")\n",
        "\n",
        "        for ct in transcripts:\n",
        "            format_and_write(ct)\n",
        "            total_fetched += 1\n",
        "\n",
        "        cursor = records.get(\"cursor\")\n",
        "        if not cursor:\n",
        "            break\n",
        "\n",
        "        batch_num += 1\n",
        "        time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "    print(f\"\\nDone — wrote {total_fetched} transcript files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TzJ9u5Nr9GYL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzJ9u5Nr9GYL",
        "outputId": "f7f4c014-1799-489b-a879-233952e5cf95"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3doq4HdS-GNr",
      "metadata": {
        "id": "3doq4HdS-GNr"
      },
      "outputs": [],
      "source": [
        "cp -r /content/gong_transcripts/* /content/drive/MyDrive/gong_transcripts/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "or0eGYZY6l6Q",
      "metadata": {
        "id": "or0eGYZY6l6Q"
      },
      "source": [
        "# DELETE EXISTING FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db02rbpQ6OZW",
      "metadata": {
        "id": "db02rbpQ6OZW"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "folder_path = \"/content/ttt\"\n",
        "shutil.rmtree(folder_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "or0eGYZY6l6Q"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
